{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0ni2OYZ8qM8"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns; sns.set_theme()\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AzO6a2F6JVpa"
   },
   "outputs": [],
   "source": [
    "#download mnist\n",
    "#choose Fashion(2) or Digit(1)\n",
    "mnist = fetch_openml('mnist_784') #(1)\n",
    "# mnist = fetch_openml(name=\"Fashion-MNIST\") #(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmPhMdVY7iuC"
   },
   "outputs": [],
   "source": [
    "binarized_fashion_mnist = (np.array(fashion_mnist.data) > 0.5).astype(np.int_)\n",
    "binarized_fashion_mnist = torch.from_numpy(binarized_fashion_mnist).float()#.transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtXo9Rt9fDKe",
    "outputId": "d86828e9-8478-44f1-a4a9-0699cfcaed62"
   },
   "outputs": [],
   "source": [
    "seed = 69\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHw40zNaJK4N"
   },
   "outputs": [],
   "source": [
    "#proprocess mnist (binarize)\n",
    "binarized_mnist = (np.array(mnist.data) > 0.5).astype(np.int_)\n",
    "binarized_mnist = torch.from_numpy(binarized_mnist).float()#.transpose(0,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIaD5CjDH3Gg"
   },
   "source": [
    "# Abstract VAE Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYEDaTUl-KCm"
   },
   "outputs": [],
   "source": [
    "class GeneralVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneralVAE, self).__init__()\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        #torch.exp(log_var) * randn(size(mu)) + mu\n",
    "        return sample\n",
    " \n",
    "    @abc.abstractmethod\n",
    "    def encoder(self, x):\n",
    "        pass \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decoder(self, z):\n",
    "        pass \n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, x):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feZSl6AwH7uo"
   },
   "source": [
    "# Simple VAE\n",
    "From A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7jB01rv_BTk"
   },
   "outputs": [],
   "source": [
    "class SimpleVAE(GeneralVAE):\n",
    "    def __init__(self):\n",
    "        super(SimpleVAE, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=784, out_features=500)\n",
    "        self.enc2 = nn.Linear(in_features=500, out_features=4)\n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=2, out_features=500)\n",
    "        self.dec2 = nn.Linear(in_features=500, out_features=784)\n",
    " \n",
    "    def decoder(self, z):\n",
    "        x = self.dec1(z)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dec2(x)\n",
    "        return x\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.enc2(x)\n",
    "        return x.view(-1, 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # get mu and log_var\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        # decoder\n",
    "        reconstruction = F.softmax(self.decoder(z),dim=-1)\n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpoPqQFePfRx"
   },
   "source": [
    "# ConvVAE\n",
    "Convolutional layers for the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LTxKDAwPeTy"
   },
   "outputs": [],
   "source": [
    "class ConvVAE(GeneralVAE):\n",
    "    def __init__(self):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        # encoder\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.enc2 = nn.Linear(in_features=5760, out_features=4)\n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=2, out_features=500)\n",
    "        self.dec2 = nn.Linear(in_features=500, out_features=784)\n",
    " \n",
    "    def decoder(self, z):\n",
    "        x = self.dec1(z)\n",
    "        x = torch.tanh(x)\n",
    "        return self.dec2(x)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = x.reshape(-1, 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.enc2(x)\n",
    "        return x.view(-1, 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # get mu and log_var\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        # decoder\n",
    "        reconstruction = F.softmax(self.decoder(z), dim=-1)\n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdznL5oSPq0Q"
   },
   "source": [
    "# DeepVAE\n",
    "VAE with more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsXnKnhoPqL_"
   },
   "outputs": [],
   "source": [
    "class DeepVAE(GeneralVAE):\n",
    "    def __init__(self):\n",
    "        super(DeepVAE, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=784, out_features=500)\n",
    "        self.enc2 = nn.Linear(in_features=500, out_features=250)\n",
    "        self.enc3 = nn.Linear(in_features=250, out_features=100)\n",
    "        self.enc4 = nn.Linear(in_features=100, out_features=50)\n",
    "        self.enc5 = nn.Linear(in_features=50, out_features=4)\n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=2, out_features=50)\n",
    "        self.dec2 = nn.Linear(in_features=50, out_features=100)\n",
    "        self.dec3 = nn.Linear(in_features=100, out_features=250)\n",
    "        self.dec4 = nn.Linear(in_features=250, out_features=500)\n",
    "        self.dec5 = nn.Linear(in_features=500, out_features=784)\n",
    " \n",
    "    def decoder(self, z):\n",
    "        x = self.dec1(z)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dec2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dec3(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dec4(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dec5(x)\n",
    "        return x\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.enc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.enc3(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.enc4(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.enc5(x)\n",
    "        return x.view(-1, 2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # get mu and log_var\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "\n",
    "        # decoder\n",
    "        reconstruction = F.softmax(self.decoder(z), dim=-1)\n",
    "        return reconstruction, mu, log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlkFCtyuPqwD"
   },
   "source": [
    "# TraditionalAE\n",
    "Traditional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S89m9naeejKE"
   },
   "outputs": [],
   "source": [
    "class TraditionalAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TraditionalAE, self).__init__()\n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(in_features=784, out_features=500)\n",
    "        self.enc2 = nn.Linear(in_features=500, out_features=250)\n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(in_features=250, out_features=500)\n",
    "        self.dec2 = nn.Linear(in_features=500, out_features=784)\n",
    " \n",
    "    def decoder(self, z):\n",
    "        x = self.dec1(z)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dec2(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.enc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # decoder\n",
    "        reconstruction = F.softmax(self.decoder(x), dim=-1)\n",
    "        return reconstruction, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ogVe2k2HquV"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNL1bCOyFj5E"
   },
   "outputs": [],
   "source": [
    "def bernoulli_log_density(x, logit_means):\n",
    "    \"\"\"\n",
    "    Numerically stable log_likelihood under bernoulli by accepting μ/(1-μ)\n",
    "    \"\"\"\n",
    "    b = x * 2 - 1  # [0, 1] -> [-1, 1]\n",
    "    return -torch.log1p(-b * logit_means)\n",
    "\n",
    "\n",
    "def log_prior(z):\n",
    "    pi = np.pi\n",
    "    return torch.sum(\n",
    "            -(z**2) / 2 - 1 / 2 * np.log(2 * pi),\n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "\n",
    "def log_q(z, q_μ, q_logσ):\n",
    "    pi = np.pi\n",
    "    return torch.sum(\n",
    "            -np.log(2 * pi) / 2 - q_logσ - (z - q_μ)**2 / (2 * torch.exp(q_logσ)**2), \n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "\n",
    "def log_likelihood(x, logit_means):\n",
    "    \"\"\" Compute log likelihood log_p(x|z)\"\"\"\n",
    "    return torch.sum(bernoulli_log_density(x, logit_means), dim=1)\n",
    "\n",
    "\n",
    "def joint_log_density(x, z, logit_means):\n",
    "    return log_likelihood(x, logit_means) + log_prior(z)\n",
    "\n",
    "\n",
    "def elbo(x, q_μ, q_logσ, z, logit_means):\n",
    "    # variational parameters from data\n",
    "    joint_ll = joint_log_density(x, z, logit_means)\n",
    "    # likelihood of z under variational distribution\n",
    "    \n",
    "    log_q_z = log_q(z, q_μ, q_logσ)\n",
    "    elbo_estimate = torch.mean(joint_ll - log_q_z)  # mean over batch\n",
    "    return elbo_estimate\n",
    "\n",
    "\n",
    "def reverse_torch_logit(logit):\n",
    "    return torch.sigmoid(1 / (1 / torch.exp(logit) + 1), dim=-1)\n",
    "\n",
    "\n",
    "def reverse_numpy_logit(logit):\n",
    "    return scipy.special.expit(1 / (1 / np.exp(logit) + 1))\n",
    "\n",
    "\n",
    "def loss_fn1(x, q_μ, q_logσ, z, logit_means):  # A3 loss function\n",
    "    return -elbo(x, q_μ, q_logσ, z, logit_means)\n",
    "\n",
    "\n",
    "def loss_fn(x, reconstruction, mu=None, log_var=None):\n",
    "    reconst_loss = F.binary_cross_entropy(reconstruction, x)\n",
    "    loss_func = nn.BCELoss(reduction='sum')\n",
    "    reconst_loss = loss_func(reconstruction, x)\n",
    "    if mu is None or log_var is None:\n",
    "        return reconst_loss\n",
    "\n",
    "    # KL divergence\n",
    "    kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Calculation error (reconstruction error and KL divergence value)\n",
    "    return reconst_loss + kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkohRVz9EUht"
   },
   "outputs": [],
   "source": [
    "def train(model, data, nepochs=100, lr=0.0001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    all_loss = []\n",
    "    for epoch in tqdm(range(nepochs)):\n",
    "        epoch_loss = 0\n",
    "        for batch in data:\n",
    "            x = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, mu, log_var, z = model(x)\n",
    "            loss = loss_fn(x, reconstruction, mu, log_var)\n",
    "            if loss.isnan().any():\n",
    "                raise ValueError(\"NaN loss\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        epoch_loss /= len(data)\n",
    "        all_loss.append(epoch_loss)\n",
    "    return all_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9IzTZHFIEAJ"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJCTb6Cse64O"
   },
   "outputs": [],
   "source": [
    "#update parameters for all models\n",
    "batch_size=500\n",
    "num_epochs = 750\n",
    "lr = 1e-4\n",
    "\n",
    "model_path=\".\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvBj-GqkVDsA"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(binarized_mnist, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQiMf4518qsL"
   },
   "outputs": [],
   "source": [
    "dataloader_fashion = torch.utils.data.DataLoader(binarized_fashion_mnist, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGNbT485VHf8",
    "outputId": "3818d506-e66f-4931-e570-24b973ccc4ef"
   },
   "outputs": [],
   "source": [
    "model_simple = SimpleVAE()\n",
    "simple_loss = train(model_simple, dataloader, nepochs=num_epochs, lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "bLUizfxyd6sb",
    "outputId": "3df59bf0-9d2a-4aa0-c302-f6a6d4f869c0"
   },
   "outputs": [],
   "source": [
    "torch.save(model_simple.state_dict(), './simple_model')\n",
    "plt.plot(simple_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e053Z2YVHvs",
    "outputId": "8ae583bd-04c9-46be-e0bc-1b46fe617c32"
   },
   "outputs": [],
   "source": [
    "model_conv = ConvVAE()\n",
    "conv_loss = train(model_conv, dataloader, nepochs=num_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "xh7cXtI1eo0O",
    "outputId": "3aabbf74-7df2-494f-a8db-4df3c664e436"
   },
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(), './model_conv')\n",
    "plt.plot(conv_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOKVyhJ-VH9I",
    "outputId": "21a2b296-f771-4178-d414-13d9678a8ad4"
   },
   "outputs": [],
   "source": [
    "model_deep = DeepVAE()\n",
    "deep_loss = train(model_deep, dataloader, nepochs=num_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "id": "4_5Aa1tEet7r",
    "outputId": "128671c8-3092-49f5-ba47-799b7056f1b9"
   },
   "outputs": [],
   "source": [
    "torch.save(model_deep.state_dict(), './model_deep')\n",
    "plt.plot(deep_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7HWna-1KtpO",
    "outputId": "200f46e6-e1b5-4d6e-c341-47fff523e98f"
   },
   "outputs": [],
   "source": [
    "model_trad = TraditionalAE()\n",
    "trad_loss = train(model_trad, dataloader, nepochs=num_epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "8yBU4uNjezDb",
    "outputId": "ce1f0828-ab48-4302-937a-6799b1561116"
   },
   "outputs": [],
   "source": [
    "torch.save(model_trad.state_dict(), './model_trad')\n",
    "plt.plot(trad_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e_qczF6IIFv"
   },
   "source": [
    "# Plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pcX5UzNWVeR"
   },
   "outputs": [],
   "source": [
    "def plot_grid(recon, num_row, num_column):\n",
    "    f, axarr = plt.subplots(num_row,num_column)\n",
    "    for i in range(0, num_row * num_column):\n",
    "        rand_i = np.random.randint(recon.shape[0])\n",
    "        img = recon[rand_i].reshape((28,28)).cpu().detach().numpy()\n",
    "        axarr[i//num_column, i%num_column].imshow(img, cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "\n",
    "def lattice_plot(model, num_row=20):\n",
    "    # f, axarr = plt.subplots(num_row,num_row)\n",
    "    # img = np.zeros((num_row*28,num_row*28))\n",
    "    img = np.zeros((num_row*28,num_row*28))\n",
    "    for i in range(num_row):\n",
    "        for j in range(num_row):\n",
    "            z = torch.tensor([i,j])/num_row*2 -1\n",
    "            means = F.softmax(model.decoder(z.to(device)),dim=-1)\n",
    "            img[i*28:i*28+28,j*28:j*28+28] = means.reshape((28,28)).cpu().detach().numpy()\n",
    "            # print(means.reshape((28,28)).cpu().detach().numpy())\n",
    "            # sns.heatmap(means.reshape((28,28)).cpu().detach().numpy())\n",
    "\n",
    "\n",
    "    # return sns.heatmap(img)\n",
    "    plt.imshow(img, cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "            # axarr[i, j].imshow(img, cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXbAnzpzVkOR"
   },
   "outputs": [],
   "source": [
    "recon_data = binarized_mnist.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "-dc6Z7cnp1mr",
    "outputId": "031ea204-388d-493e-f868-c79b628395ce"
   },
   "outputs": [],
   "source": [
    "recons_simple, mu_simple, log_var_simple, z_simple = model_simple(recon_data)\n",
    "plot_grid(recons_simple, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "KYxhovr5jotF",
    "outputId": "53396643-2add-4537-f500-d09467e7104c"
   },
   "outputs": [],
   "source": [
    "\n",
    "lattice_plot(model_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "aa-hK7JIXWP0",
    "outputId": "78c09e2a-4ca5-45e9-9e51-72a1bc6b43a7"
   },
   "outputs": [],
   "source": [
    "recons_conv, mu_conv, log_var_conv, z_conv = model_conv(recon_data)\n",
    "plot_grid(recons_conv, 4, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "-EDXeKnEjd5E",
    "outputId": "38d335ab-9909-4a15-cc21-2961513bea90"
   },
   "outputs": [],
   "source": [
    "\n",
    "lattice_plot(model_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "93zVGkJJXWfE",
    "outputId": "bf26191a-ac94-46fb-cdf4-bd53195ab73e"
   },
   "outputs": [],
   "source": [
    "recons_deep, mu_deep, log_var_deep, z_deep = model_deep(recon_data)\n",
    "plot_grid(recons_deep, 4, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "id": "EnI7b1Nmjagr",
    "outputId": "895f7aae-9423-47ab-aa83-5dab8a81cf7b"
   },
   "outputs": [],
   "source": [
    "\n",
    "lattice_plot(model_deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "grOG_sg-XWrz",
    "outputId": "584f2cab-a45e-447d-c2f8-c91c02a4bdc2"
   },
   "outputs": [],
   "source": [
    "recons_trad, mu_trad, log_var_trad, z_trad = model_trad(recon_data)\n",
    "plot_grid(recons_trad, 4, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run for memory issues when running on local cuda\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vae.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
